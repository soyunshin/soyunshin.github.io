---
title:  "[Dacon 01] 신용카드 사기 거래 탐지 AI 경진대회 (작성중)"
excerpt: "Unsupervised anomaly detection for Credit card fraud detection'"

categories:
  - Data Analytics
tags:
  - [Data Analytics]

toc: true
toc_sticky: true
 
date: 2022-08-23
last_modified_at: 2022-08-23
---

# 0. 문제 정의

본 데이터는 신용카드의 사기 거래를 탐지하기 위한 신용카드 거래 데이터이다.

분석 목표는 신용카드 <mark style = 'background-color:#fff5b1'>사기 거래의 여부를 모르는</mark> 방대한 데이터로도 사기 거래를 탐지할 수 있는 AI 모델을 개발하는 것이다.

라벨이 주어지지 않는 비지도학습 문제이다.
{: .notice--info}

<br>

해당 데이터에 변수는 ID, V1, $ \cdots $, V30의 30개 컬럼을 가진 비식별화된 거래 데이터가 들어있다.

<br>

사용 Model

 - Auto Encoder

<br>


# 1. 필요한 환경 설정
```python
import random
import pandas as pd
import numpy as np
import os
import math
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm.auto import tqdm
from sklearn.metrics import f1_score
```


```python
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
```

## 1.1 Hyperparameter

```python
EPOCHS = 50
LR = 1e-2
BS = 2 ** 14
SEED = 1234
```

`seed` 값 고정

```python
def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True

seed_everything(SEED) # Seed 고정
```

<br>


# 2. Custom Dataset

데이터셋 불러오기

```python
train_df = pd.read_csv('./Data/train.csv')
train_df = train_df.drop(columns=['ID'])

val_df = pd.read_csv('./Data/val.csv')
val_df = val_df.drop(columns=['ID'])

test_df = pd.read_csv('./Data/test.csv')
test_df = test_df.drop(columns=['ID'])
```

커스텀 데이터로더 작성

```python
class Cardstrans(Dataset):
    def __init__(self, df, eval_mode):
        self.df = df
        self.eval_mode = eval_mode
        if self.eval_mode:
            self.labels = self.df['Class'].values
            self.df = self.df.drop(columns=['Class']).values
        else:
            self.df = self.df.values

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        if self.eval_mode:
            self.x = self.df[index]
            self.y = self.labels[index]
            return torch.Tensor(self.x), self.y
        else:
            self.x = self.df[index]
            return torch.Tensor(self.x)
```



```python
train_dataset = Cardstrans(df=train_df, eval_mode=False)
train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True, num_workers=4)

val_dataset = Cardstrans(df = val_df, eval_mode=True)
val_loader = DataLoader(val_dataset, batch_size=BS, shuffle=False, num_workers=4)

test_dataset = Cardstrans(test_df, False)
test_loader = DataLoader(test_dataset, batch_size=BS, shuffle=False, num_workers=4)
```

<br>

# 3. Model

모델 성능 개선을 위한 도구

- Layer Normalization
- GELU
- Skip connection

```python
class LayerNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-5):
        """Construct a layernorm module in the TF style (epsilon inside the square root).
        """
        super(LayerNorm, self).__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.bias = nn.Parameter(torch.zeros(hidden_size))
        self.variance_epsilon = eps

        self.init_weights()

    def init_weights(self):
        self.weight.data.fill_(1.0)
        self.bias.data.zero_()

    def forward(self, x):
        u = x.mean(-1, keepdim=True)
        s = (x - u).pow(2).mean(-1, keepdim=True)
        x = (x - u) / torch.sqrt(s + self.variance_epsilon) # Layer Normalization 

        return self.weight * x + self.bias # wx+b
```

```python
class GELU(nn.Module):
    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))
```

<br>

Model 정의 : Auto Encoder


```python
class AutoEncoder1(nn.Module):
    def __init__(self):
        super(AutoEncoder1, self).__init__()
        
        
        self.ln = LayerNorm(5000)
        self.ln1 = LayerNorm(3500)
        self.ln2 = LayerNorm(2000)
        self.ln3 = LayerNorm(1000)
        
        self.upblock1 = nn.Sequential(nn.Linear(30, 1000), nn.BatchNorm1d(1000),GELU())
        self.upblock2 = nn.Sequential(nn.Linear(1000,2000), nn.BatchNorm1d(2000),GELU())
        self.upblock3 = nn.Sequential(nn.Linear(2000,3500), nn.BatchNorm1d(3500),GELU())
        self.upblock4 = nn.Sequential(nn.Linear(3500,5000), nn.BatchNorm1d(5000),GELU())

        self.downblock1 = nn.Sequential(nn.Linear(5000, 3500),nn.BatchNorm1d(3500),GELU())
        self.downblock2 = nn.Sequential(nn.Linear(3500, 2000),nn.BatchNorm1d(2000),GELU())
        self.downblock3 = nn.Sequential(nn.Linear(2000, 1000),nn.BatchNorm1d(1000),GELU())
        self.downblock4 = nn.Sequential(nn.Linear(1000, 300),nn.BatchNorm1d(300),GELU())
        
        self.fclayer = nn.Sequential(nn.Linear(300,30))
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        upblock1_out = self.upblock1(x) 
        upblock2_out = self.upblock2(upblock1_out)
        upblock3_out = self.upblock3(upblock2_out)
        upblock4_out = self.upblock4(upblock3_out)
        
        downblock1_out = self.downblock1(self.ln(upblock4_out)) 
        skipblock1 = downblock1_out + upblock3_out
        downblock2_out = self.downblock2(self.ln1(skipblock1))
        skipblock2 = downblock2_out + upblock2_out
        downblock3_out = self.downblock3(self.ln2(skipblock2))
        skipblock3 = downblock3_out + upblock1_out 
        downblock4_out = self.downblock4(self.ln3(skipblock3))
        
        x = self.fclayer(downblock4_out)
         
        return x 
```

<br>

# 4. Train

모델 학습을 위한 `Trainer` 클래스 정의

```python
class Trainer():
    def __init__(self, model, optimizer, train_loader, val_loader, scheduler, device):
        self.model = model
        self.optimizer = optimizer
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.scheduler = scheduler
        self.device = device
        # Loss Function
        self.criterion = nn.L1Loss().to(self.device)
        
    def fit(self):
        self.model.to(self.device)
        best_score = 0
        avg = 1
        for epoch in range(EPOCHS):
            self.model.train()
            train_loss = []
            for x in iter(self.train_loader):
                x = x.float().to(self.device)
                self.optimizer.zero_grad()

                _x = self.model(x)
                loss = self.criterion(x, _x)

                loss.backward()
                self.optimizer.step()

                train_loss.append(loss.item())

            score = self.validation(self.model, 0.95)
            print(f'Epoch : [{epoch}] Train loss : [{np.mean(train_loss)}] Val Score : [{score}])')

            if self.scheduler is not None:
                self.scheduler.step(score)

            if best_score <= score and avg > np.mean(train_loss):
                best_score = score
                avg = np.mean(train_loss)
                torch.save(model.module.state_dict(), './best_model.pth', _use_new_zipfile_serialization=False)
                print('---------------------------')
                print(f'Train loss : [{np.mean(train_loss)}] Val Score : [{score}])')
    
    def validation(self, eval_model, thr):
        cos = nn.CosineSimilarity(dim=1, eps=1e-6)
        eval_model.eval()
        pred = []
        true = []
        with torch.no_grad():
            for x, y in iter(self.val_loader):
                x = x.float().to(self.device)

                _x = self.model(x)
                diff = cos(x, _x).cpu().tolist()
                batch_pred = np.where(np.array(diff)<thr, 1, 0).tolist()
                pred += batch_pred
                true += y.tolist()

        return f1_score(true, pred, average='macro')
```


```python
import torch, gc
gc.collect()
torch.cuda.empty_cache()
```


```python
import warnings
warnings.filterwarnings(action='ignore')

model = nn.DataParallel(AutoEncoder1())
model.eval()
optimizer = torch.optim.Adam(params = model.parameters(), lr = LR)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, threshold_mode='abs', min_lr=1e-8, verbose=True)
trainer = Trainer(model, optimizer, train_loader, val_loader, scheduler, device)
trainer.fit()
```


```python
model = AutoEncoder1()
model.load_state_dict(torch.load('./best_model.pth'))
model = nn.DataParallel(model)
model.eval()
```

<br>

# 5. Prediction

```python
def prediction(model, thr, test_loader, device):
    model.to(device)
    model.eval()
    cos = nn.CosineSimilarity(dim=1, eps=1e-6)
    pred = []
    with torch.no_grad():
        for x in iter(test_loader):
            x = x.float().to(device)
            _x = model(x)
            
            diff = cos(x, _x).cpu().tolist()
            batch_pred = np.where(np.array(diff)<thr, 1,0).tolist()
            pred += batch_pred
    return pred
```


`test` 데이터를 통해 결과 생성

```python
preds = prediction(model, 0.95, test_loader, device)
```

```python
submit = pd.read_csv('./Data/sample_submission.csv')
submit['Class'] = preds
```


클래스별 데이터 개수는 다음과 같다.

```python
submit['Class'].value_counts()
```

|label|count|
|:---:|---:|
|0 |    142190|
|1 |      313 |


<br>

결과는 다음과 같다.
|public | private |
|:---:|:---:|
|0.9305289388 | 0.9098817409 |