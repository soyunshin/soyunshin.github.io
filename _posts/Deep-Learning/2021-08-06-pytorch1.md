---
title:  "[Pytorch 01] Linear Regression"
excerpt: "Linear Regression 문제를 Pytorch를 통해 풀이"

categories:
  - Deep Learning
tags:
  - [Deep Learning, Pytorch]

toc: true
toc_sticky: true
 
date: 2021-08-06
last_modified_at: 2021-08-06
---

Linear Regression 문제를 Pytorch를 통해 풀어본다.
<br>
<br>
# 0. 필요한 Module import

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
```

random seed를 설정해 같은 결과를 얻도록 고정할 수도 있다.
```python
torch.manual_seed(1)
```

<br>
<br>

# 1. simple
다음과 같은 가설의 문제를 푸는 선형 회귀 모델을 Pytorch를 사용해서 만들어본다.

$$ H(x)= Wx+b $$

train 데이터셋은 다음과 같이 구성한다.
```python
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[3], [5], [7]])
```

weight와 bias는 다음과 같이 0으로 초기화한다.
```python
W = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```

optimizer를 정한다. 여기서는 SGD optimizer를 사용하도록 한다.
```python
optimizer = optim.SGD([W, b], lr=0.01)
```


epoch 수 만큼 계산을 반복하여, Cost 값이 줄어드는 방향의 weight와 bias를 찾도록 한다.

```python
epochs = 1000 
for epoch in range(epochs + 1):

    # H(x) 계산
    hypothesis = x_train * W + b

    # cost 계산
    cost = torch.mean((hypothesis - y_train) ** 2)

    # cost를 통해 update
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    # 100번마다 로그 출력
    if epoch % 100 == 0:
        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(
            epoch, epochs, W.item(), b.item(), cost.item()
        ))
```

결과는 다음과 같다.

![image](https://user-images.githubusercontent.com/70592135/128490125-7c4eedce-b031-4a43-b00a-10bb3f527b10.png)

<br>
<br>
<br>

# 2. multiple

### 2-1. 다항식
다음과 같은 가설을 정의한다.
$$
H(x) = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + b
$$

<br>

train 데이터셋은 다음과 같이 구성한다.

```python
x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])
x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])
x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])
```

weight와 bias는 다음과 같이 0으로 초기화한다.

```python
w1 = torch.zeros(1, requires_grad=True)
w2 = torch.zeros(1, requires_grad=True)
w3 = torch.zeros(1, requires_grad=True)

b = torch.zeros(1, requires_grad=True)
```


optimizer를 정한다. 여기서는 SGD optimizer를 사용하도록 한다.

```python
optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)
```

epoch 수 만큼 계산을 반복하여, Cost 값이 줄어드는 방향의 weight와 bias를 찾도록 한다.

```python
epochs = 1000

for epoch in range(epochs + 1):

    # H(x) 계산
    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b

    # cost 계산
    cost = torch.mean((hypothesis - y_train) ** 2)

    # cost를 통해 H(x) update
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    # 200번마다 출력
    if epoch % 200 == 0:
        print('Epoch {:4d}/{} \nw1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} \nCost: {:.6f}'.format(
            epoch, epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()
        ))
        print()
```
결과는 다음과 같다.
![image](https://user-images.githubusercontent.com/70592135/128491762-2bdf2752-28bf-40d2-83b5-2f96138c2ab4.png)


<br>
<br>

---
<br>

### 2-2. 행렬
입력 데이터를 행렬 형태로 선언하는 것도 가능하다.
$$H(X) = XW + B$$


train 데이터셋은 다음과 같이 행렬형태로 구성한다.

```python
x_train  =  torch.FloatTensor([[73,  80,  75], 
                               [93,  88,  93], 
                               [89,  91,  80], 
                               [96,  98,  100],   
                               [73,  66,  70]])  
y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])
```

크기는 다음과 같다.
```python
print(x_train.shape)
print(y_train.shape)
```
![image](https://user-images.githubusercontent.com/70592135/128491870-7d275570-9f67-46c2-8ca5-0777b4376cf1.png)


weight와 bias는 다음과 같이 train data과 연산이 가능한 형태의 행렬로, 0 값을 가지도록 초기화한다.

```python
W = torch.zeros((3, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```

가설은 다음과 같이 행렬 곱셈을 통해 구성한다.
```python
hypothesis = x_train.matmul(W) + b
```



optimizer를 정한다. 여기서는 SGD optimizer를 사용하도록 한다.

```python
optimizer = optim.SGD([W, b], lr=1e-5)
```

epoch 수 만큼 계산을 반복하여, Cost 값이 줄어드는 방향의 weight와 bias를 찾도록 한다.


```python
epochs = 1000
for epoch in range(epochs + 1):

    # H(x) 계산
    # bias는 브로드 캐스팅되어 각 샘플에 더해진다.
    hypothesis = x_train.matmul(W) + b

    # cost 계산
    cost = torch.mean((hypothesis - y_train) ** 2)

    # cost로 H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    if epoch % 200 == 0:
      print('Epoch {:4d}/{} \nHypothesis: {} \nCost: {:.6f}'.format(
          epoch, epochs, hypothesis.squeeze().detach(), cost.item()
          ))
      print()
```
![image](https://user-images.githubusercontent.com/70592135/128492669-ca98c716-265c-4290-88f6-f0bf12c2a387.png)



