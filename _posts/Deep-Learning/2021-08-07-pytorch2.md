---
title:  "[Pytorch 02] Multi Layer Perceptron 01"
excerpt: "MNIST classification 문제를 MLP 구조를 사용해 풀이"

categories:
  - Deep Learning
tags:
  - [Deep Learning, Pytorch]

toc: true
toc_sticky: true
 
date: 2021-08-07
last_modified_at: 2021-08-07
---


MNIST classification 문제를 MLP 구조를 사용하여 풀어본다.
<br>
<br>

# 0. 환경 설정

## 0-1. 필요한 Module import

```python
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms, datasets
```

## 0-2. model을 설계할 때 활용하는 장비 확인

```python
if torch.cuda.is_available() :
  DEVICE = torch.device('cuda')
else : 
  DEVICE = torch.device('cpu')

print('Using PyTorch version : ', torch.__version__, 'Device : ',DEVICE)
```
구글 colab에서 사용하는 Pytorch와 cuda 버전은 다음과 같다.
![image](https://user-images.githubusercontent.com/70592135/128587882-0da1f195-2336-4030-963f-82664fa59620.png)

Batch size와 Epochs 수는 다음과 같이 정한다.

```python
BATCH_SIZE = 32
EPOCHS = 10
```


# 1. Data

`torchvision`내 `datasets`함수를 사용해서 MNIST dataset을 다운로드한다. <br>
train과 test를 구분하고, `transforms.ToTensor()`메서드를 사용해, `tensor`형태로 변경하고, 0~255사이의 픽셀값을 0~1범위로 정규화한다.
```python
train_dataset = datasets.MNIST(root = "/MNIST",
                               train = True,
                               download = True,
                               transform = transforms.ToTensor())

test_dataset = datasets.MNIST(root = "/MNIST",
                              train = False,
                              transform = transforms.ToTensor())
```

데이터를 Mini-Batch 단위로 분리한다. `DataLoader`함수를 이용하고, train data의 경우는 shuffle을 하지만, test data는 하지 않는다.
```python
train_loader = torch.utils.data.DataLoader(dataset = train_dataset,
                                           batch_size = BATCH_SIZE,
                                           shuffle = True)

test_loader = torch.utils.data.DataLoader(dataset = test_dataset,
                                          batch_size = BATCH_SIZE,
                                          shuffle = False)
```



`DataLoader`를 통해 데이터가 어떻게 할당되었는지 확인한다.
각 Mini-Batch당 다음과 같은 차원의 tensor로 구성된다.
[ 32개의 이미지, 0 or 1로 이루어진 흑백이미지, 가로28개 픽셀, 세로28개 픽셀] 

```python
for(X_train, y_train) in train_loader :
  print('X_train : ',X_train.size(), 'type : ', X_train.type())
  print('y_train : ',y_train.size(), 'type : ', y_train.type())
  break
```
![image](https://user-images.githubusercontent.com/70592135/128588112-47d9981a-1c69-462f-b423-5889f137dbee.png)


어떤 이미지인지 확인하면 다음과 같다.
```python
pltsize = 1
plt.figure(figsize = (10 * pltsize, pltsize))
for i in range(10) :
  plt.subplot(1, 10, i + 1)
  plt.axis('off')
  plt.imshow(X_train[i,:,:,:].numpy().reshape(28,28), cmap="gray_r")
  plt.title('Class : ' + str(y_train[i].item()))
```
![image](https://user-images.githubusercontent.com/70592135/128588416-ed407663-f1d5-47c6-aac6-b9101d1182a5.png)


# 2. Model

사용할 모델은 다음과 같다.<br>
`nn.Module` 내에 있는 메서드를 상속받아 이용한다.
여기서 `x.view` 이 부분은 Flatten을 진행하는 부분이다.
<br>
MLP모델은 1차원의 벡터값을 입력으로 받을 수 있다. 그러나 MNIST 이미지 데이터는 크기가 28*28인 2차원 데이터이므로 이를 1차원으로 변환한다.

## 2-1. 기본

```python
class Net(nn.Module) :
  def __init__(self) :
    super(Net, self).__init__() 
    self.fc1 = nn.Linear(28*28, 512)
    self.fc2 = nn.Linear(512, 256)
    self.fc3 = nn.Linear(256,10)

  def forward(self, x) :
    x = x.view(-1, 28 * 28) 
    x = self.fc1(x)
    x = F.sigmoid(x)
    x = self.fc2(x)
    x = F.sigmoid(x)
    x = self.fc3(x)
    x = F.log_softmax(x, dim=1)
    return x

```

## 2-2. Dropout + ReLU + Batch Normalization

위와 같은 기본 구조에, 다음과 같이 Dropout과 다른 activation 함수, Batch Normalization를 적용할 수 있다.
<br>Batch Normalization이란, Layer의 Input의 분포를 정규화해 학습 속도를 빠르게 하는 방법으로,
각 Layer마다 Input의 분포가 달라짐에 따라 학습 속도가 느려지는 Internal Covariance shift라는 문제를 방지하는 기법이다.<br>

$$BN(h;\gamma,\beta) = \beta + \gamma \frac{h-E(h)}{\sqrt{Var(h) + \epsilon}}$$

h는 input 분포를 의미하고,
beta, gamma는 각각 분포를 shift, scaling 시키는 값으로 Back Propagation을 통해 학습되는 parameter이다.<br>


여기서는 각 Layer에서 1D 크기의 벡터를 계산하기 때문에 `nn.BatchNorm1d()`를 사용한다.



```python
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 10)
        self.dropout_prob = 0.5
        self.batch_norm1 = nn.BatchNorm1d(512)
        self.batch_norm2 = nn.BatchNorm1d(256)

    def forward(self, x):
        x = x.view(-1, 28 * 28)
        x = self.fc1(x)
        x = self.batch_norm1(x)
        x = F.relu(x)
        x = F.dropout(x, training = self.training, p = self.dropout_prob)
        x = self.fc2(x)
        x = self.batch_norm2(x)
        x = F.relu(x)
        x = F.dropout(x, training = self.training, p = self.dropout_prob)
        x = self.fc3(x)
        x = F.log_softmax(x, dim = 1)
        return x
```


모델은 기존에 설정한 `Device`에 할당하고, 
파라미터를 업데이트할 때 사용할 Optimizer를 정의한다.
여기서는 `SGD`를 사용한다.<br>
MLP모델의 output과 계산할 Label값은 각 숫자 클래스를 표현하는 one-hot 인코딩 값이다.
둘의 차이는 `CrossEntropy`를 사용하여 계산한다.
```python
model = Net().to(DEVICE)
optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)
criterion = nn.CrossEntropyLoss() #loss
```

2-1. 기본 MLP 모델은 다음과 같은 layer를 가진다.
```python
print(model)
```
![image](https://user-images.githubusercontent.com/70592135/128588735-c1cdc08d-4245-4684-a0c9-26b9289af6b1.png)


2-2. 모델은 다음과 같은 layer를 가진다.
```python
print(model)
```
![image](https://user-images.githubusercontent.com/70592135/128589096-0fa872bd-ac9d-4d4c-8415-d014993203be.png)

# 3. Train, Evaluation

모델 학습을 진행하고, train data에 대하여 모델의 성능을 확인하는 함수를 정의한다.
정의한 모델을 `model.train()`상태로 지정한다.
```python
def train(model, train_loader, optimizer, log_interval) :
  model.train()
  for batch_idx, (image, label) in enumerate(train_loader) :
    image = image.to(DEVICE)
    label = label.to(DEVICE)
    optimizer.zero_grad()
    output = model(image)
    loss = criterion(output, label)
    loss.backward()
    optimizer.step()

    if batch_idx % log_interval == 0 :
      print("Train Epoch : {} [{}/{}({:.0f}%)]\t Train Loss : {:.6f}".format(epoch, batch_idx * len(image),
                                                                             len(train_loader.dataset), 100. * batch_idx / len(train_loader),
                                                                             loss.item()))
```


학습 과정에서 test data에 대한 모델 성능을 확인하는 함수를 정의한다.
학습이 완료된 모델을 `model.eval()` 즉, 평가하는 상태로 지정한다.

평가 단계에서는 Gradient를 통해 파라미터가 업데이트되는 것을 방지하기 위해, `torch.no_grad()`메서드를 사용해 Gradient의 흐름을 제어한다.

`prediction`에는 앞 서 정의한 모델의 마지막 `softmax`로 계산된 벡터 내에서 가장 큰 값의 해당 위치에 대응하는 클래스로 예측했다고 판단한다.
```python
def evaluate(model, test_loader) :
  model.eval()
  test_loss = 0
  correct = 0

  with torch.no_grad():
    for image, label in test_loader : 
      image = image.to(DEVICE)
      label = label.to(DEVICE)
      output = model(image)
      test_loss += criterion(output, label).item() # loss
      prediction = output.max(1, keepdim = True)[1] 
      correct += prediction.eq(label.view_as(prediction)).sum().item()

    test_loss /= len(test_loader.dataset)
    test_accuracy = 100.  *correct / len(test_loader.dataset)
    return test_loss, test_accuracy
```

# 4. Result

epoch수 만큼 학습을 진행한다.
```python
for epoch in range(1, EPOCHS + 1):
    train(model, train_loader, optimizer, log_interval = 200)
    test_loss, test_accuracy = evaluate(model, test_loader)
    print("\n[EPOCH: {}], \tTest Loss: {:.4f}, \tTest Accuracy: {:.2f} % \n".format(
        epoch, test_loss, test_accuracy))
```
2-1 모델의 최종 결과는 다음과 같다.
![image](https://user-images.githubusercontent.com/70592135/128588762-1e517bf0-9ba0-4ec3-a3b5-d05b1e679ac5.png)

2-2 모델의 최종 결과는 다음과 같다.
![image](https://user-images.githubusercontent.com/70592135/128589149-1323d919-0151-4aac-ba69-8664db0d9d0d.png)